{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_filenames(path_to_data):\n",
    "    return sorted([file_name for file_name in os.listdir(path_to_data) \n",
    "                        if file_name.endswith('.jpg')], key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    \n",
    "def check_data_equality(distorted_filenames, clean_filenames):\n",
    "    try:\n",
    "        if distorted_filenames != clean_filenames:\n",
    "            raise ValueError('Filenames for distorted and clean data are not the same.')\n",
    "    except ValueError as e:\n",
    "        print(str(e) + ' Making them equal.')\n",
    "        distorted_filenames = sorted(list(set(distorted_filenames) & set(clean_filenames)), key=lambda x: int(x.split('.')[0]))\n",
    "        clean_filenames = distorted_filenames\n",
    "    return distorted_filenames, clean_filenames\n",
    "    \n",
    "    \n",
    "distorted_train_path = 'data/train/distorted/'\n",
    "clean_train_path = 'data/train/clean/'\n",
    "\n",
    "distorted_validation_path = 'data/validation/distorted/'\n",
    "clean_validation_path = 'data/validation/clean/'\n",
    "\n",
    "distorted_train_filenames, clean_train_filenames = check_data_equality(\n",
    "    get_filenames(distorted_train_path + '/images'), get_filenames(clean_train_path + '/images'))\n",
    "\n",
    "distorted_validation_filenames, clean_validation_filenames = check_data_equality(\n",
    "    get_filenames(distorted_validation_path + '/images'), get_filenames(clean_validation_path + '/images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def create_dataset(data_path: str) -> torch.utils.data.Dataset:\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, distorted_dataset, clean_dataset):\n",
    "        self.distorted_dataset = distorted_dataset\n",
    "        self.clean_dataset = clean_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.distorted_dataset[index][0], self.clean_dataset[index][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_train_dataset = create_dataset(distorted_train_path)\n",
    "clean_train_dataset = create_dataset(clean_train_path)\n",
    "train_dataset = CustomImageDataset(distorted_train_dataset, clean_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_validation_dataset = create_dataset(distorted_validation_path)\n",
    "clean_validation_dataset = create_dataset(clean_validation_path)\n",
    "validation_dataset = CustomImageDataset(distorted_validation_dataset, clean_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64 ,kernel_size=3 , stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=32 ,kernel_size=2 , stride=1, padding=1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=8 ,kernel_size=2 , stride=1, padding=1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=8, out_channels=32, kernel_size=2, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=2, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 46, 46]           1,792\n",
      "              ReLU-2           [-1, 64, 46, 46]               0\n",
      "         MaxPool2d-3           [-1, 64, 15, 15]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           8,224\n",
      "         MaxPool2d-5             [-1, 32, 5, 5]               0\n",
      "              ReLU-6             [-1, 32, 5, 5]               0\n",
      "            Conv2d-7              [-1, 8, 6, 6]           1,032\n",
      "         MaxPool2d-8              [-1, 8, 2, 2]               0\n",
      "              ReLU-9              [-1, 8, 2, 2]               0\n",
      "         Upsample-10              [-1, 8, 6, 6]               0\n",
      "  ConvTranspose2d-11             [-1, 32, 5, 5]           1,056\n",
      "             ReLU-12             [-1, 32, 5, 5]               0\n",
      "         Upsample-13           [-1, 32, 15, 15]               0\n",
      "  ConvTranspose2d-14           [-1, 64, 16, 16]           8,256\n",
      "             ReLU-15           [-1, 64, 16, 16]               0\n",
      "         Upsample-16           [-1, 64, 48, 48]               0\n",
      "  ConvTranspose2d-17            [-1, 3, 48, 48]           1,731\n",
      "             ReLU-18            [-1, 3, 48, 48]               0\n",
      "================================================================\n",
      "Total params: 22,091\n",
      "Trainable params: 22,091\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 3.80\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 3.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "autoencoder = ConvolutionalAutoencoder()\n",
    "summary(autoencoder, input_size=(3, 48, 48))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,input_out, target_out):\n",
    "        loss = torch.sqrt(self.mse(input_out, target_out), dim=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_test_path = 'data/test/'\n",
    "test_dataset = create_dataset(distorted_test_path)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory: model/ as it does not exist\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "def create_not_existing_directory(directory: str):\n",
    "    \"\"\"\n",
    "    Create not existing directory. \n",
    "    If directory exists, do nothing.\n",
    "    :param directory: str\n",
    "        directory to create\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(directory)\n",
    "    if not p.is_dir():\n",
    "        print(f'Creating directory: {directory} as it does not exist')\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "create_not_existing_directory('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.000799836739897728\n",
      "Epoch: 1, loss: 0.0006796955931931734\n",
      "Epoch: 2, loss: 0.0005925298482179641\n",
      "Epoch: 3, loss: 0.0005557763753458858\n",
      "Epoch: 4, loss: 0.0005256142355501652\n",
      "Epoch: 5, loss: 0.0005135449860244989\n",
      "Epoch: 6, loss: 0.0004885858483612538\n",
      "Epoch: 7, loss: 0.0004788570562377572\n",
      "Epoch: 8, loss: 0.00047068750206381084\n",
      "Epoch: 9, loss: 0.0004605470346286893\n",
      "Epoch: 10, loss: 0.0004561832258477807\n",
      "Epoch: 11, loss: 0.00045033457968384025\n",
      "Epoch: 12, loss: 0.000442861994728446\n",
      "Epoch: 13, loss: 0.0004491222715005279\n",
      "Epoch: 14, loss: 0.00043758621625602243\n",
      "Epoch: 15, loss: 0.0004352013487368822\n",
      "Epoch: 16, loss: 0.0004378717867657542\n",
      "Epoch: 17, loss: 0.0004343781741335988\n",
      "Epoch: 18, loss: 0.00043423171527683736\n",
      "Epoch: 19, loss: 0.00043422760255634784\n",
      "Epoch: 20, loss: 0.00042700533289462327\n",
      "Epoch: 21, loss: 0.0004225705415010452\n",
      "Epoch: 22, loss: 0.00042593732383102177\n",
      "Epoch: 23, loss: 0.00042542371340095997\n",
      "Epoch: 24, loss: 0.0004220482613891363\n",
      "Epoch: 25, loss: 0.0004241406563669443\n",
      "Epoch: 26, loss: 0.00042918355204164983\n",
      "Epoch: 27, loss: 0.0004179746098816395\n",
      "Epoch: 28, loss: 0.00041526890639215707\n",
      "Epoch: 29, loss: 0.00041938507463783024\n",
      "Epoch: 30, loss: 0.0004146803487092257\n",
      "Epoch: 31, loss: 0.0004211736135184765\n",
      "Epoch: 32, loss: 0.00040946643054485323\n",
      "Epoch: 33, loss: 0.00041179132275283336\n",
      "Epoch: 34, loss: 0.0004157502604648471\n",
      "Epoch: 35, loss: 0.00042182835470885036\n",
      "Epoch: 36, loss: 0.000412431781180203\n",
      "Epoch: 37, loss: 0.0004091362180188298\n",
      "Epoch: 38, loss: 0.00040633718203753234\n",
      "Epoch: 39, loss: 0.00040544659085571766\n",
      "Epoch: 40, loss: 0.00040503996703773737\n",
      "Epoch: 41, loss: 0.0004073004499077797\n",
      "Epoch: 42, loss: 0.00040179013647139073\n",
      "Epoch: 43, loss: 0.0004042929643765092\n",
      "Epoch: 44, loss: 0.00040020480938255786\n",
      "Epoch: 45, loss: 0.000400573399849236\n",
      "Epoch: 46, loss: 0.00040140033978968857\n",
      "Epoch: 47, loss: 0.0004024882335215807\n",
      "Epoch: 48, loss: 0.0004008554695174098\n",
      "Epoch: 49, loss: 0.00040105394925922153\n",
      "Epoch: 50, loss: 0.0004022092595696449\n",
      "Epoch: 51, loss: 0.0003980502039194107\n",
      "Epoch: 52, loss: 0.00039858294837176797\n",
      "Epoch: 53, loss: 0.0004048933647572994\n",
      "Epoch: 54, loss: 0.0003946353429928422\n",
      "Epoch: 55, loss: 0.00039471385162323714\n",
      "Epoch: 56, loss: 0.0003962624277919531\n",
      "Epoch: 57, loss: 0.00039631166867911815\n",
      "Epoch: 58, loss: 0.00039402168523520233\n",
      "Epoch: 59, loss: 0.0003943912899121642\n",
      "Epoch: 60, loss: 0.00039964355528354645\n",
      "Epoch: 61, loss: 0.0003949073618277907\n",
      "Epoch: 62, loss: 0.00039514642022550107\n",
      "Epoch: 63, loss: 0.0003928903033956885\n",
      "Epoch: 64, loss: 0.0003969515869393945\n",
      "Epoch: 65, loss: 0.0003904475448653102\n",
      "Epoch: 66, loss: 0.0003957289960235357\n",
      "Epoch: 67, loss: 0.0003935038605704904\n",
      "Epoch: 68, loss: 0.000391506371088326\n",
      "Epoch: 69, loss: 0.0003935704603791237\n",
      "Epoch: 70, loss: 0.00039435127563774584\n",
      "Epoch: 71, loss: 0.00038949436880648133\n",
      "Epoch: 72, loss: 0.00039317371137440206\n",
      "Epoch: 73, loss: 0.00038652939256280663\n",
      "Epoch: 74, loss: 0.0003908917931839824\n",
      "Epoch: 75, loss: 0.00038758762925863267\n",
      "Epoch: 76, loss: 0.0003896655896678567\n",
      "Epoch: 77, loss: 0.0003869803100824356\n",
      "Epoch: 78, loss: 0.00038529191818088295\n",
      "Epoch: 79, loss: 0.0003892967589199543\n",
      "Epoch: 80, loss: 0.0003857550574466586\n",
      "Epoch: 81, loss: 0.00038331612292677165\n",
      "Epoch: 82, loss: 0.00038383470196276905\n",
      "Epoch: 83, loss: 0.00038523314148187636\n",
      "Epoch: 84, loss: 0.0003885331721976399\n",
      "Epoch: 85, loss: 0.00038367228396236897\n",
      "Epoch: 86, loss: 0.000390157756395638\n",
      "Epoch: 87, loss: 0.0003829878233373165\n",
      "Epoch: 88, loss: 0.00038467098120599985\n",
      "Epoch: 89, loss: 0.000382916540838778\n",
      "Epoch: 90, loss: 0.0003972514104098082\n",
      "Epoch: 91, loss: 0.00038175280671566726\n",
      "Epoch: 92, loss: 0.00038201523572206495\n",
      "Epoch: 93, loss: 0.0003830327345058322\n",
      "Epoch: 94, loss: 0.0003812864450737834\n",
      "Epoch: 95, loss: 0.00038256215304136276\n",
      "Epoch: 96, loss: 0.0003858867455273867\n",
      "Epoch: 97, loss: 0.0003821346862241626\n",
      "Epoch: 98, loss: 0.0003881067121401429\n",
      "Epoch: 99, loss: 0.0003831312339752913\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "epochs_without_improvement: int = 0\n",
    "max_epochs_without_improvement: int = 8\n",
    "the_lowest_loss: int = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        _, decoded_out = autoencoder(x)\n",
    "        loss_value = loss(y, decoded_out)\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value_per_epoch = 0\n",
    "        for x, y in validation_dataloader:\n",
    "            _, decoded_out = autoencoder(x)\n",
    "            loss_value = loss(y, decoded_out)\n",
    "            loss_value_per_epoch = loss_value_per_epoch + loss_value.item()\n",
    "        \n",
    "        loss_value_per_epoch = loss_value_per_epoch / len(distorted_validation_dataset)\n",
    "        print(f\"Epoch: {epoch}, loss: {loss_value_per_epoch}\")\n",
    "        if loss_value_per_epoch < the_lowest_loss:\n",
    "            the_lowest_loss = loss_value_per_epoch\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(autoencoder.state_dict(), os.path.join('model', 'model' + '.pt'))\n",
    "        else:\n",
    "            epochs_without_improvement = epochs_without_improvement + 1\n",
    "            if epochs_without_improvement == max_epochs_without_improvement:\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest loss: 0.0003812864450737834\n"
     ]
    }
   ],
   "source": [
    "print(f'The lowest loss: {the_lowest_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = ConvolutionalAutoencoder()\n",
    "autoencoder.load_state_dict(torch.load(os.path.join('model', 'model' + '.pt')))\n",
    "with torch.no_grad():\n",
    "    decoded_images = []\n",
    "    for x in test_dataloader:\n",
    "        _, decoded_out = autoencoder(x[0])\n",
    "        decoded_images.append(decoded_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_test = torch.cat(decoded_images, dim=0)\n",
    "stacked_test = stacked_test.reshape(-1, 3, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_result(images: np.ndarray, out_path: str):\n",
    "    \n",
    "    assert images.shape == (400, 3, 48, 48)\n",
    "    \n",
    "    flat_img = images.reshape(400, -1)\n",
    "    n_rows = np.prod(images.shape)\n",
    "    \n",
    "    y_with_id = np.concatenate([np.arange(n_rows).reshape(-1, 1), flat_img.reshape(n_rows, 1)], axis=1)\n",
    "    np.savetxt(out_path, y_with_id, delimiter=\",\", fmt=['%d', '%.4f'], header=\"id,expetced\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory: results/ as it does not exist\n"
     ]
    }
   ],
   "source": [
    "create_not_existing_directory('results/')\n",
    "save_result(stacked_test.detach().numpy(), 'results/result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
