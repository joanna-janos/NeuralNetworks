{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image denoising\n",
    "The aim of this project is to prepare neural network which takes images with noise and returns clean one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data equality\n",
    "Firstly, I want to check if all files in distorted directory have corresponding clean images. If not, data will be equalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_filenames(path_to_data):\n",
    "    return sorted([file_name for file_name in os.listdir(path_to_data) \n",
    "                        if file_name.endswith('.jpg')], key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    \n",
    "def check_data_equality(distorted_filenames, clean_filenames):\n",
    "    try:\n",
    "        if distorted_filenames != clean_filenames:\n",
    "            raise ValueError('Filenames for distorted and clean data are not the same.')\n",
    "    except ValueError as e:\n",
    "        print(str(e) + ' Making them equal.')\n",
    "        distorted_filenames = sorted(list(set(distorted_filenames) & set(clean_filenames)), key=lambda x: int(x.split('.')[0]))\n",
    "        clean_filenames = distorted_filenames\n",
    "    return distorted_filenames, clean_filenames\n",
    "    \n",
    "    \n",
    "distorted_train_path = 'data/train/distorted/'\n",
    "clean_train_path = 'data/train/clean/'\n",
    "\n",
    "distorted_validation_path = 'data/validation/distorted/'\n",
    "clean_validation_path = 'data/validation/clean/'\n",
    "\n",
    "distorted_train_filenames, clean_train_filenames = check_data_equality(\n",
    "    get_filenames(distorted_train_path + '/images'), get_filenames(clean_train_path + '/images'))\n",
    "\n",
    "distorted_validation_filenames, clean_validation_filenames = check_data_equality(\n",
    "    get_filenames(distorted_validation_path + '/images'), get_filenames(clean_validation_path + '/images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data as tensors will be stored in datasets prepared for working with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def create_dataset(data_path: str) -> torch.utils.data.Dataset:\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PyTorch does not have dataset for images as input and output, I have prepared custom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, distorted_dataset, clean_dataset):\n",
    "        self.distorted_dataset = distorted_dataset\n",
    "        self.clean_dataset = clean_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.distorted_dataset[index][0], self.clean_dataset[index][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split (train & validation)\n",
    "Data from distorted and clean directories was splitted into train and validation.\n",
    "Train consists of 80% of examples. Other 20% is assigned to validation.\n",
    "I splitted data manually - first two hundreds of each thousand is taken into validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_train_dataset = create_dataset(distorted_train_path)\n",
    "clean_train_dataset = create_dataset(clean_train_path)\n",
    "train_dataset = CustomImageDataset(distorted_train_dataset, clean_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_validation_dataset = create_dataset(distorted_validation_path)\n",
    "clean_validation_dataset = create_dataset(clean_validation_path)\n",
    "validation_dataset = CustomImageDataset(distorted_validation_dataset, clean_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "As a model I chose convolutional autoencoder. <br>\n",
    "It consists of two parts: encoder (convolution + max pooling) and decoder (transpose convolution + upsampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64 ,kernel_size=3 , stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=32 ,kernel_size=2 , stride=1, padding=1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=8 ,kernel_size=2 , stride=1, padding=1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=8, out_channels=32, kernel_size=2, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=2, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=3, mode='bilinear', align_corners=True),\n",
    "            torch.nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchsummary\n",
    "`torchsummary` provides nice summary of model and output shape after each layer. It was useful during creation of decoder while having encoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 46, 46]           1,792\n",
      "              ReLU-2           [-1, 64, 46, 46]               0\n",
      "         MaxPool2d-3           [-1, 64, 15, 15]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           8,224\n",
      "         MaxPool2d-5             [-1, 32, 5, 5]               0\n",
      "              ReLU-6             [-1, 32, 5, 5]               0\n",
      "            Conv2d-7              [-1, 8, 6, 6]           1,032\n",
      "         MaxPool2d-8              [-1, 8, 2, 2]               0\n",
      "              ReLU-9              [-1, 8, 2, 2]               0\n",
      "         Upsample-10              [-1, 8, 6, 6]               0\n",
      "  ConvTranspose2d-11             [-1, 32, 5, 5]           1,056\n",
      "             ReLU-12             [-1, 32, 5, 5]               0\n",
      "         Upsample-13           [-1, 32, 15, 15]               0\n",
      "  ConvTranspose2d-14           [-1, 64, 16, 16]           8,256\n",
      "             ReLU-15           [-1, 64, 16, 16]               0\n",
      "         Upsample-16           [-1, 64, 48, 48]               0\n",
      "  ConvTranspose2d-17            [-1, 3, 48, 48]           1,731\n",
      "             ReLU-18            [-1, 3, 48, 48]               0\n",
      "================================================================\n",
      "Total params: 22,091\n",
      "Trainable params: 22,091\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 3.80\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 3.91\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "autoencoder = ConvolutionalAutoencoder()\n",
    "summary(autoencoder, input_size=(3, 48, 48))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE loss\n",
    "As RMSE loss will be used for evaluating models, I implemented class which can be used during training to observe validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,input_out, target_out):\n",
    "        loss = torch.sqrt(self.mse(input_out, target_out) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_test_path = 'data/test/'\n",
    "test_dataset = create_dataset(distorted_test_path)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils for creating not existing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def create_not_existing_directory(directory: str):\n",
    "    \"\"\"\n",
    "    Create not existing directory. \n",
    "    If directory exists, do nothing.\n",
    "    :param directory: str\n",
    "        directory to create\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(directory)\n",
    "    if not p.is_dir():\n",
    "        print(f'Creating directory: {directory} as it does not exist')\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "create_not_existing_directory('model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training is conducted for 100 epochs and can be stopped if there is no improvements for loss value (should decrease) in following 8 epochs.\n",
    "\n",
    "The best model so far is saved for predictions purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.0035788718909025193\n",
      "Epoch: 1, loss: 0.0033174886927008628\n",
      "Epoch: 2, loss: 0.0030996461808681488\n",
      "Epoch: 3, loss: 0.0030359924733638763\n",
      "Epoch: 4, loss: 0.0029196545854210856\n",
      "Epoch: 5, loss: 0.002964951179921627\n",
      "Epoch: 6, loss: 0.002847883924841881\n",
      "Epoch: 7, loss: 0.002823504202067852\n",
      "Epoch: 8, loss: 0.002795414872467518\n",
      "Epoch: 9, loss: 0.002812020227313042\n",
      "Epoch: 10, loss: 0.002781913921236992\n",
      "Epoch: 11, loss: 0.0027661576345562934\n",
      "Epoch: 12, loss: 0.0027250415235757827\n",
      "Epoch: 13, loss: 0.0027250478565692903\n",
      "Epoch: 14, loss: 0.002702657103538513\n",
      "Epoch: 15, loss: 0.002700040742754936\n",
      "Epoch: 16, loss: 0.002687894992530346\n",
      "Epoch: 17, loss: 0.002682480715215206\n",
      "Epoch: 18, loss: 0.0026886307448148726\n",
      "Epoch: 19, loss: 0.002677454359829426\n",
      "Epoch: 20, loss: 0.0026683810502290727\n",
      "Epoch: 21, loss: 0.002658243544399738\n",
      "Epoch: 22, loss: 0.0026534142345190047\n",
      "Epoch: 23, loss: 0.002647374853491783\n",
      "Epoch: 24, loss: 0.002648480162024498\n",
      "Epoch: 25, loss: 0.002630806252360344\n",
      "Epoch: 26, loss: 0.0026366979256272315\n",
      "Epoch: 27, loss: 0.0026372070088982584\n",
      "Epoch: 28, loss: 0.0026767098382115364\n",
      "Epoch: 29, loss: 0.0026236722618341447\n",
      "Epoch: 30, loss: 0.002611665450036526\n",
      "Epoch: 31, loss: 0.002638784758746624\n",
      "Epoch: 32, loss: 0.0026119653210043906\n",
      "Epoch: 33, loss: 0.002602276258170605\n",
      "Epoch: 34, loss: 0.0026110562458634377\n",
      "Epoch: 35, loss: 0.0026032101213932035\n",
      "Epoch: 36, loss: 0.002671634040772915\n",
      "Epoch: 37, loss: 0.0026033096089959143\n",
      "Epoch: 38, loss: 0.0025909701734781266\n",
      "Epoch: 39, loss: 0.0025891710370779036\n",
      "Epoch: 40, loss: 0.002621829807758331\n",
      "Epoch: 41, loss: 0.0025848949030041696\n",
      "Epoch: 42, loss: 0.0025723274201154707\n",
      "Epoch: 43, loss: 0.0025787570998072624\n",
      "Epoch: 44, loss: 0.002590869225561619\n",
      "Epoch: 45, loss: 0.002569085776805878\n",
      "Epoch: 46, loss: 0.002571093536913395\n",
      "Epoch: 47, loss: 0.002569505989551544\n",
      "Epoch: 48, loss: 0.00259261205047369\n",
      "Epoch: 49, loss: 0.0025770119801163676\n",
      "Epoch: 50, loss: 0.0026299323067069053\n",
      "Epoch: 51, loss: 0.00257747557759285\n",
      "Epoch: 52, loss: 0.002566157080233097\n",
      "Epoch: 53, loss: 0.0025766363963484764\n",
      "Epoch: 54, loss: 0.002571337252855301\n",
      "Epoch: 55, loss: 0.002555788181722164\n",
      "Epoch: 56, loss: 0.0025641578808426858\n",
      "Epoch: 57, loss: 0.002563198298215866\n",
      "Epoch: 58, loss: 0.002568746447563171\n",
      "Epoch: 59, loss: 0.002555063523352146\n",
      "Epoch: 60, loss: 0.0025572885647416116\n",
      "Epoch: 61, loss: 0.002551894173026085\n",
      "Epoch: 62, loss: 0.0025483846217393873\n",
      "Epoch: 63, loss: 0.0025484961941838265\n",
      "Epoch: 64, loss: 0.002569823823869228\n",
      "Epoch: 65, loss: 0.0025441780164837838\n",
      "Epoch: 66, loss: 0.002548827439546585\n",
      "Epoch: 67, loss: 0.0025325470864772798\n",
      "Epoch: 68, loss: 0.0025502368956804274\n",
      "Epoch: 69, loss: 0.0025630070865154267\n",
      "Epoch: 70, loss: 0.0025663693621754647\n",
      "Epoch: 71, loss: 0.0025372730046510695\n",
      "Epoch: 72, loss: 0.002535225972533226\n",
      "Epoch: 73, loss: 0.002544128976762295\n",
      "Epoch: 74, loss: 0.0025297718495130538\n",
      "Epoch: 75, loss: 0.002532011292874813\n",
      "Epoch: 76, loss: 0.002547113239765167\n",
      "Epoch: 77, loss: 0.002530257225036621\n",
      "Epoch: 78, loss: 0.002559238813817501\n",
      "Epoch: 79, loss: 0.0025246544480323793\n",
      "Epoch: 80, loss: 0.0025534855350852013\n",
      "Epoch: 81, loss: 0.0025249983966350556\n",
      "Epoch: 82, loss: 0.0025338196158409118\n",
      "Epoch: 83, loss: 0.0025516293942928316\n",
      "Epoch: 84, loss: 0.002547544628381729\n",
      "Epoch: 85, loss: 0.002527541287243366\n",
      "Epoch: 86, loss: 0.002527237921953201\n",
      "Epoch: 87, loss: 0.002520062193274498\n",
      "Epoch: 88, loss: 0.002515949219465256\n",
      "Epoch: 89, loss: 0.002520301580429077\n",
      "Epoch: 90, loss: 0.0025364480316638948\n",
      "Epoch: 91, loss: 0.0025034353882074354\n",
      "Epoch: 92, loss: 0.0025096751153469085\n",
      "Epoch: 93, loss: 0.0025031174868345263\n",
      "Epoch: 94, loss: 0.00250825959444046\n",
      "Epoch: 95, loss: 0.002541195288300514\n",
      "Epoch: 96, loss: 0.0025130425468087195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-18c83d47edb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2cd65e55fbfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2cd65e55fbfd>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn.{} is deprecated. Use nn.functional.interpolate instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nn2019/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but linear mode needs 3D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bilinear2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2448\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 150\n",
    "loss = RMSELoss()\n",
    "\n",
    "epochs_without_improvement: int = 0\n",
    "max_epochs_without_improvement: int = 8\n",
    "the_lowest_loss: int = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        _, decoded_out = autoencoder(x)\n",
    "        loss_value = loss(y, decoded_out)\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value_per_epoch = 0\n",
    "        for x, y in validation_dataloader:\n",
    "            _, decoded_out = autoencoder(x)\n",
    "            loss_value = loss(y, decoded_out)\n",
    "            loss_value_per_epoch = loss_value_per_epoch + loss_value.item()\n",
    "        \n",
    "        loss_value_per_epoch = loss_value_per_epoch / len(distorted_validation_dataset)\n",
    "        print(f\"Epoch: {epoch}, loss: {loss_value_per_epoch}\")\n",
    "        if loss_value_per_epoch < the_lowest_loss:\n",
    "            the_lowest_loss = loss_value_per_epoch\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(autoencoder.state_dict(), os.path.join('model', 'model' + '.pt'))\n",
    "        else:\n",
    "            epochs_without_improvement = epochs_without_improvement + 1\n",
    "            if epochs_without_improvement == max_epochs_without_improvement:\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest loss: 0.0025031174868345263\n"
     ]
    }
   ],
   "source": [
    "print(f'The lowest loss: {the_lowest_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "The best model is used for preapring predictions for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = ConvolutionalAutoencoder()\n",
    "autoencoder.load_state_dict(torch.load(os.path.join('model', 'model' + '.pt')))\n",
    "with torch.no_grad():\n",
    "    decoded_images = []\n",
    "    for x in test_dataloader:\n",
    "        _, decoded_out = autoencoder(x[0])\n",
    "        decoded_images.append(decoded_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_test = torch.cat(decoded_images, dim=0)\n",
    "stacked_test = stacked_test.reshape(-1, 3, 48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results \n",
    "Results are saved to csv file matching kaggle submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_result(images: np.ndarray, out_path: str):\n",
    "    \n",
    "    assert images.shape == (400, 3, 48, 48)\n",
    "    \n",
    "    flat_img = images.reshape(400, -1)\n",
    "    n_rows = np.prod(images.shape)\n",
    "    \n",
    "    y_with_id = np.concatenate([np.arange(n_rows).reshape(-1, 1), flat_img.reshape(n_rows, 1)], axis=1)\n",
    "    np.savetxt(out_path, y_with_id, delimiter=\",\", fmt=['%d', '%.4f'], header=\"id,expetced\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_not_existing_directory('results/')\n",
    "save_result(stacked_test.detach().numpy(), 'results/result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
